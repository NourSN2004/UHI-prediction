{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaZ2p7C_rBBL",
        "outputId": "f1ee954e-3b03-4b0d-b595-6fe1aa7a265b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqTwXLCaG3yG",
        "outputId": "2a067917-8844-45f7-8887-cdbd8a4b644f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# === 1) Mount Drive & imports ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, math, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# install timm and torchvision if missing\n",
        "!pip install timm torchvision\n",
        "\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "!pip install timm torchvision tifffile --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCCOAUVUXKL5"
      },
      "outputs": [],
      "source": [
        "# 0) Install dependencies (including LZW support for tifffile)\n",
        "!pip install timm torchvision tifffile imagecodecs --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lhbe4nIUVuAG"
      },
      "outputs": [],
      "source": [
        "# 0) Install dependencies\n",
        "!pip install timm torchvision rasterio --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZmjv3rMJLpZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn import MSELoss\n",
        "\n",
        "import timm\n",
        "from tifffile import imread        # ← lightweight TIFF reader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZUW7b3PHQxI"
      },
      "outputs": [],
      "source": [
        "class LSTDataset(Dataset):\n",
        "    def __init__(self, df, patches_dir, weather_cols):\n",
        "        self.df           = df.reset_index(drop=True)\n",
        "        self.patches_dir  = patches_dir\n",
        "        self.weather_cols = weather_cols\n",
        "        self.transform    = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485,0.456,0.406],\n",
        "                std =[0.229,0.224,0.225],\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row  = self.df.loc[idx]\n",
        "        path = os.path.join(self.patches_dir, row[\"filename\"])\n",
        "\n",
        "        # read all bands via tifffile; yields shape (bands, H, W)\n",
        "        arr = imread(path).astype(np.float32)\n",
        "\n",
        "        # ── bands 2,3,4 for RGB input ──\n",
        "        img_np = arr[[1,2,3], :, :].transpose(1,2,0).astype(np.uint8)\n",
        "        img    = self.transform(img_np)\n",
        "\n",
        "        # ── band 1 as LST target ──\n",
        "        tar_np = arr[0, :, :]\n",
        "        target = torch.tensor(tar_np, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # ── weather as float32 tensor ──\n",
        "        weather = torch.tensor(\n",
        "            row[self.weather_cols].values.astype(np.float32)\n",
        "        )\n",
        "\n",
        "        return img, weather, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff0OdZcLHW9d"
      },
      "outputs": [],
      "source": [
        "class PretrainedViTLSTModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 weather_dim: int = 5,\n",
        "                 hidden_dim:  int = 768,\n",
        "                 vit_name:    str = \"vit_base_patch16_224\",\n",
        "                 num_layers:  int = 2,\n",
        "                 num_heads:   int = 8):\n",
        "        super().__init__()\n",
        "        # pretrained ViT (no head)\n",
        "        self.vit = timm.create_model(\n",
        "            vit_name,\n",
        "            pretrained=True,\n",
        "            num_classes=0\n",
        "        )\n",
        "        for p in self.vit.parameters(): p.requires_grad = False\n",
        "\n",
        "        # weather → embedding\n",
        "        self.weather_proj = nn.Linear(weather_dim, hidden_dim)\n",
        "\n",
        "        # small Transformer to fuse tokens + weather\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim*4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\n",
        "\n",
        "        # deconv head back to 1‑channel map\n",
        "        p = getattr(self.vit.patch_embed, \"patch_size\", 16)\n",
        "        self.deconv = nn.ConvTranspose2d(hidden_dim, 1,\n",
        "                                         kernel_size=p, stride=p)\n",
        "\n",
        "    def forward(self, images, weather):\n",
        "        feats  = self.vit.forward_features(images)  # [B,1+N,D]\n",
        "        tokens = feats[:,1:,:]                      # [B,N,D]\n",
        "        w_emb  = self.weather_proj(weather).unsqueeze(1)  # [B,1,D]\n",
        "        tkns   = torch.cat([tokens, w_emb], dim=1)         # [B,N+1,D]\n",
        "\n",
        "        t = tkns.permute(1,0,2)   # [seq,B,D]\n",
        "        t = self.transformer(t)\n",
        "        t = t.permute(1,0,2)      # [B,seq,D]\n",
        "\n",
        "        patch_out = t[:,:-1,:]    # [B,N,D]\n",
        "        B,N,D     = patch_out.shape\n",
        "        G         = int(math.sqrt(N))\n",
        "        x         = patch_out.transpose(1,2).view(B,D,G,G)  # [B,D,G,G]\n",
        "\n",
        "        return self.deconv(x)     # [B,1,G*p,G*p]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjmNOIqIHack",
        "outputId": "1ff3a779-e14a-4216-9f6a-97eb989c2515"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "weather_cols = [\n",
        "    \"air_temp_C\", \"dew_point_C\",\n",
        "    \"relative_humidity_percent\",\n",
        "    \"wind_speed_m_s\", \"precipitation_in\"\n",
        "]\n",
        "\n",
        "model = PretrainedViTLSTModel(\n",
        "    weather_dim=len(weather_cols),\n",
        "    hidden_dim=768,\n",
        "    vit_name=\"vit_base_patch16_224\",\n",
        "    num_layers=2,\n",
        "    num_heads=8\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A5oL3fUSHbd",
        "outputId": "ba759608-8f28-406c-dc93-d2275bad8770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['filename', 'date', 'datetime', 'air_temp_C', 'dew_point_C',\n",
            "       'relative_humidity_percent', 'wind_speed_m_s', 'precipitation_in'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# — run once and check the output in Colab —\n",
        "df          = pd.read_csv(\"/content/drive/MyDrive/PatchedOutput/tiff_with_meteo.csv\")\n",
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnnrOd50Hqa-"
      },
      "outputs": [],
      "source": [
        "# — read your patched CSV —\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/PatchedOutput/tiff_with_meteo.csv\")\n",
        "\n",
        "# — ensure weather columns are floats —\n",
        "for col in weather_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# — drop any rows with missing weather or filename —\n",
        "df = df.dropna(subset=weather_cols + [\"filename\"]).reset_index(drop=True)\n",
        "\n",
        "patches_dir = \"/content/drive/MyDrive/PatchedOutput_Cleaned\"\n",
        "\n",
        "# — dataset & split —\n",
        "dataset  = LSTDataset(df, patches_dir, weather_cols)\n",
        "train_sz = int(0.8 * len(dataset))\n",
        "val_sz   = len(dataset) - train_sz\n",
        "train_ds, val_ds = random_split(dataset, [train_sz, val_sz])\n",
        "\n",
        "# — DataLoaders —\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=8,      # smaller batch\n",
        "    shuffle=True,\n",
        "    num_workers=0,     # safest: no background workers\n",
        "    pin_memory=False   # also turn off pin_memory\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X74-cD6QPaV_",
        "outputId": "b849346e-9df6-4a19-bfec-c4e920affc5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "opt       = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4, weight_decay=1e-5\n",
        ")\n",
        "loss_fn   = MSELoss()\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    opt, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJB4VKIrPcrE"
      },
      "outputs": [],
      "source": [
        "ckpt_dir  = \"/content/drive/MyDrive/checkpointsViT\"\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "ckpt_path = os.path.join(ckpt_dir, \"vit_lstm_ckpt.pth\")\n",
        "\n",
        "def save_ckpt(epoch):\n",
        "    torch.save({\n",
        "        \"epoch\":       epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"opt_state\":   opt.state_dict(),\n",
        "        \"sched_state\": scheduler.state_dict()\n",
        "    }, ckpt_path)\n",
        "    print(f\"✔ Saved checkpoint at epoch {epoch}\")\n",
        "\n",
        "def load_ckpt():\n",
        "    if os.path.isfile(ckpt_path):\n",
        "        ck = torch.load(ckpt_path)\n",
        "        model.load_state_dict(ck[\"model_state\"])\n",
        "        opt.load_state_dict(ck[\"opt_state\"])\n",
        "        scheduler.load_state_dict(ck[\"sched_state\"])\n",
        "        print(f\"→ Resuming from epoch {ck['epoch']}\")\n",
        "        return ck[\"epoch\"] + 1\n",
        "    return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D2Yd2aNPf7A",
        "outputId": "5ffd4f21-caba-471c-d375-e202b236c923"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 00 Train: 100%|##########| 1429/1429 [39:35<00:00,  1.66s/it, batch_loss=2.8788]\n",
            "Epoch 00  Val : 100%|##########| 358/358 [09:50<00:00,  1.65s/it, batch_loss=1.3212]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00 | Train: 25.3160 | Val: 9.8544\n",
            "✔ Saved checkpoint at epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 01 Train: 100%|##########| 1429/1429 [04:44<00:00,  5.02it/s, batch_loss=1.1887]\n",
            "Epoch 01  Val : 100%|##########| 358/358 [00:55<00:00,  6.44it/s, batch_loss=1.8620]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train: 6.7949 | Val: 5.5388\n",
            "✔ Saved checkpoint at epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 02 Train: 100%|##########| 1429/1429 [04:49<00:00,  4.93it/s, batch_loss=0.8701]\n",
            "Epoch 02  Val : 100%|##########| 358/358 [00:55<00:00,  6.48it/s, batch_loss=1.5197]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02 | Train: 3.6407 | Val: 3.0290\n",
            "✔ Saved checkpoint at epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 03 Train: 100%|##########| 1429/1429 [04:48<00:00,  4.96it/s, batch_loss=0.6319]\n",
            "Epoch 03  Val : 100%|##########| 358/358 [00:55<00:00,  6.48it/s, batch_loss=2.0159]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03 | Train: 3.7118 | Val: 2.4946\n",
            "✔ Saved checkpoint at epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 04 Train: 100%|##########| 1429/1429 [04:49<00:00,  4.94it/s, batch_loss=0.8953]\n",
            "Epoch 04  Val : 100%|##########| 358/358 [00:55<00:00,  6.45it/s, batch_loss=1.0777]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04 | Train: 1.8198 | Val: 1.2943\n",
            "✔ Saved checkpoint at epoch 4\n",
            "✅ Training finished\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F  # for F.interpolate\n",
        "\n",
        "num_epochs = 5\n",
        "start_ep   = load_ckpt()\n",
        "\n",
        "for epoch in range(start_ep, num_epochs):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_bar  = tqdm(train_loader, desc=f\"Epoch {epoch:02d} Train\", ascii=True)\n",
        "    for imgs, weather, tgt in train_bar:\n",
        "        imgs, weather, tgt = imgs.to(device), weather.to(device), tgt.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out = model(imgs, weather)\n",
        "        # ─── resize output to match tgt H×W ─────────────────────────\n",
        "        if out.shape[2:] != tgt.shape[2:]:\n",
        "            out = F.interpolate(out, size=tgt.shape[2:], mode='bilinear', align_corners=False)\n",
        "        # ──────────────────────────────────────────────────────────────\n",
        "\n",
        "        loss = loss_fn(out, tgt)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "        train_bar.set_postfix(batch_loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_bar  = tqdm(val_loader, desc=f\"Epoch {epoch:02d}  Val \", ascii=True)\n",
        "    with torch.no_grad():\n",
        "        for imgs, weather, tgt in val_bar:\n",
        "            imgs, weather, tgt = imgs.to(device), weather.to(device), tgt.to(device)\n",
        "\n",
        "            out = model(imgs, weather)\n",
        "            if out.shape[2:] != tgt.shape[2:]:\n",
        "                out = F.interpolate(out, size=tgt.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "            batch_loss = loss_fn(out, tgt).item()\n",
        "            val_loss   += batch_loss * imgs.size(0)\n",
        "            val_bar.set_postfix(batch_loss=f\"{batch_loss:.4f}\")\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    # — Scheduler & Logging —\n",
        "    scheduler.step(val_loss)\n",
        "    print(f\"Epoch {epoch:02d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
        "    save_ckpt(epoch)\n",
        "\n",
        "print(\"✅ Training finished\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bu3_ON3ENKX"
      },
      "source": [
        "Here’s what those two metrics mean in your tqdm bars:\n",
        "\n",
        "- **b‑loss**: the **batch loss** for the *current* mini‑batch—that is, the value of `loss.item()` you just computed before backprop.  \n",
        "- **avg**: the **running average loss** up through that batch within the epoch. It’s computed as  \n",
        "  \\[\n",
        "    \\text{avg} = \\frac{\\sum_{\\text{all batches so far}} \\bigl(\\text{batch\\_loss} \\times \\text{batch\\_size}\\bigr)}{\\text{(number of samples seen so far)}}\n",
        "  \\]\n",
        "  so it tells you how training (or validation) loss is trending on average as the epoch progresses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVJiX0LkDri0",
        "outputId": "fe2ed6d9-856f-4fc0-f331-15ab22f5e0bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 00 ▶ Train: 100%|##########| 1429/1429 [04:42<00:00,  5.05it/s, batch=1429/1429, b-loss=3.4007, avg=1.7620]\n",
            "Epoch 00 ◀ Val  : 100%|##########| 358/358 [00:55<00:00,  6.45it/s, batch=358/358, b-loss=1.3964, avg=3.8357]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00 Summary → Train: 1.7620 | Val: 3.8438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 01 ▶ Train: 100%|##########| 1429/1429 [04:44<00:00,  5.02it/s, batch=1429/1429, b-loss=0.6000, avg=1.5009]\n",
            "Epoch 01 ◀ Val  : 100%|##########| 358/358 [00:56<00:00,  6.37it/s, batch=358/358, b-loss=1.2923, avg=1.5352]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 Summary → Train: 1.5009 | Val: 1.5384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 02 ▶ Train: 100%|##########| 1429/1429 [04:43<00:00,  5.04it/s, batch=1429/1429, b-loss=0.5604, avg=1.4002]\n",
            "Epoch 02 ◀ Val  : 100%|##########| 358/358 [00:55<00:00,  6.43it/s, batch=358/358, b-loss=1.5912, avg=1.3513]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02 Summary → Train: 1.4002 | Val: 1.3542\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 03 ▶ Train: 100%|##########| 1429/1429 [04:42<00:00,  5.05it/s, batch=1429/1429, b-loss=0.9012, avg=0.8043]\n",
            "Epoch 03 ◀ Val  : 100%|##########| 358/358 [00:56<00:00,  6.35it/s, batch=358/358, b-loss=1.3241, avg=1.3961]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03 Summary → Train: 0.8043 | Val: 1.3990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 04 ▶ Train: 100%|##########| 1429/1429 [04:42<00:00,  5.06it/s, batch=1429/1429, b-loss=0.2675, avg=0.6305]\n",
            "Epoch 04 ◀ Val  : 100%|##########| 358/358 [00:55<00:00,  6.45it/s, batch=358/358, b-loss=1.3185, avg=0.9852]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04 Summary → Train: 0.6305 | Val: 0.9873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 05 ▶ Train: 100%|##########| 1429/1429 [04:41<00:00,  5.07it/s, batch=1429/1429, b-loss=0.4374, avg=0.5946]\n",
            "Epoch 05 ◀ Val  : 100%|##########| 358/358 [00:55<00:00,  6.48it/s, batch=358/358, b-loss=1.4934, avg=1.2413]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05 Summary → Train: 0.5946 | Val: 1.2439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 06 ▶ Train: 100%|##########| 1429/1429 [04:43<00:00,  5.04it/s, batch=1429/1429, b-loss=0.2204, avg=0.6138]\n",
            "Epoch 06 ◀ Val  : 100%|##########| 358/358 [00:55<00:00,  6.44it/s, batch=358/358, b-loss=1.0285, avg=0.7208]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06 Summary → Train: 0.6138 | Val: 0.7223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 07 ▶ Train: 100%|##########| 1429/1429 [04:42<00:00,  5.07it/s, batch=1429/1429, b-loss=0.4487, avg=0.5894]\n",
            "Epoch 07 ◀ Val  : 100%|##########| 358/358 [00:55<00:00,  6.47it/s, batch=358/358, b-loss=1.9437, avg=1.4219]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07 Summary → Train: 0.5894 | Val: 1.4249\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 08 ▶ Train: 100%|##########| 1429/1429 [04:41<00:00,  5.08it/s, batch=1429/1429, b-loss=0.4082, avg=0.5761]\n",
            "Epoch 08 ◀ Val  : 100%|##########| 358/358 [00:55<00:00,  6.50it/s, batch=358/358, b-loss=1.0682, avg=0.8165]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08 Summary → Train: 0.5761 | Val: 0.8182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 09 ▶ Train: 100%|##########| 1429/1429 [04:42<00:00,  5.06it/s, batch=1429/1429, b-loss=25.0908, avg=0.5526]\n",
            "Epoch 09 ◀ Val  : 100%|##########| 358/358 [00:55<00:00,  6.50it/s, batch=358/358, b-loss=2.0534, avg=6.8430]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09 Summary → Train: 0.5526 | Val: 6.8574\n",
            "✅ Training finished\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F  # for interpolation\n",
        "\n",
        "CKPT_PATH    = \"checkpoint.pth\"\n",
        "TOTAL_EPOCHS = 10  # total epochs you want to train\n",
        "\n",
        "def save_ckpt(epoch):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': opt.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "    }, CKPT_PATH)\n",
        "\n",
        "def load_ckpt():\n",
        "    if os.path.exists(CKPT_PATH):\n",
        "        ckpt = torch.load(CKPT_PATH, map_location=device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        opt.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "        # resume at the next epoch\n",
        "        return ckpt['epoch'] + 1\n",
        "    return 0\n",
        "\n",
        "start_ep = load_ckpt()\n",
        "\n",
        "for epoch in range(start_ep, TOTAL_EPOCHS):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch:02d} ▶ Train\", ascii=True)\n",
        "    for batch_idx, (imgs, weather, tgt) in enumerate(train_bar, 1):\n",
        "        imgs, weather, tgt = imgs.to(device), weather.to(device), tgt.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out = model(imgs, weather)\n",
        "        if out.shape[2:] != tgt.shape[2:]:\n",
        "            out = F.interpolate(out,\n",
        "                                size=tgt.shape[2:],\n",
        "                                mode='bilinear',\n",
        "                                align_corners=False)\n",
        "\n",
        "        loss = loss_fn(out, tgt)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "        avg_loss = train_loss / (batch_idx * train_loader.batch_size)\n",
        "\n",
        "        train_bar.set_postfix({\n",
        "            'batch': f\"{batch_idx}/{len(train_loader)}\",\n",
        "            'b-loss': f\"{loss.item():.4f}\",\n",
        "            'avg':    f\"{avg_loss:.4f}\"\n",
        "        })\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_bar  = tqdm(val_loader, desc=f\"Epoch {epoch:02d} ◀ Val  \", ascii=True)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, weather, tgt) in enumerate(val_bar, 1):\n",
        "            imgs, weather, tgt = imgs.to(device), weather.to(device), tgt.to(device)\n",
        "\n",
        "            out = model(imgs, weather)\n",
        "            if out.shape[2:] != tgt.shape[2:]:\n",
        "                out = F.interpolate(out,\n",
        "                                    size=tgt.shape[2:],\n",
        "                                    mode='bilinear',\n",
        "                                    align_corners=False)\n",
        "\n",
        "            batch_loss = loss_fn(out, tgt).item()\n",
        "            val_loss   += batch_loss * imgs.size(0)\n",
        "            avg_val    = val_loss / (batch_idx * val_loader.batch_size)\n",
        "\n",
        "            val_bar.set_postfix({\n",
        "                'batch': f\"{batch_idx}/{len(val_loader)}\",\n",
        "                'b-loss': f\"{batch_loss:.4f}\",\n",
        "                'avg':    f\"{avg_val:.4f}\"\n",
        "            })\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    # — Scheduler & Checkpoint —\n",
        "    scheduler.step(val_loss)\n",
        "    print(f\"Epoch {epoch:02d} Summary → Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
        "    save_ckpt(epoch)\n",
        "\n",
        "print(\"✅ Training finished\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9d1reslaiAy"
      },
      "source": [
        "We add random crops, flips, rotations and color jitters to expose the model to more varied inputs, which helps it generalize better rather than memorizing the exact patches. This augmentation reduces overfitting and makes your ViT‑based head more robust to real‑world variability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g749ozUqabA4"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "class LSTDataset(Dataset):\n",
        "    def __init__(self, df, patches_dir, weather_cols):\n",
        "        self.df           = df.reset_index(drop=True)\n",
        "        self.patches_dir  = patches_dir\n",
        "        self.weather_cols = weather_cols\n",
        "        # stronger augmentations for better generalization\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(degrees=15),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std =[0.229, 0.224, 0.225],\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row  = self.df.loc[idx]\n",
        "        path = os.path.join(self.patches_dir, row[\"filename\"])\n",
        "\n",
        "        # read all bands; arr shape = (bands, H, W)\n",
        "        arr    = imread(path).astype(np.float32)\n",
        "        img_np = arr[[1,2,3],:,:].transpose(1,2,0).astype(np.uint8)\n",
        "        img    = self.transform(img_np)\n",
        "\n",
        "        # band 1 as LST target\n",
        "        tar_np = arr[0,:,:]\n",
        "        target = torch.tensor(tar_np, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        weather = torch.tensor(\n",
        "            row[self.weather_cols].values.astype(np.float32)\n",
        "        )\n",
        "\n",
        "        return img, weather, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dQCXhHlamqS",
        "outputId": "639ec479-69a7-47f9-e4ea-e38f37721737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports & setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, math, time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F           # ← added\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "from tifffile import imread\n",
        "\n",
        "!pip install timm torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRSPXhQ_aqv8"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Updated Dataset (now resizes target as well)\n",
        "class LSTDataset(Dataset):\n",
        "    def __init__(self, df, patches_dir, weather_cols):\n",
        "        self.df           = df.reset_index(drop=True)\n",
        "        self.patches_dir  = patches_dir\n",
        "        self.weather_cols = weather_cols\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
        "            transforms.RandomHorizontalFlip(0.5),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "                                 std =[0.229,0.224,0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row  = self.df.loc[idx]\n",
        "        path = os.path.join(self.patches_dir, row[\"filename\"])\n",
        "\n",
        "        arr    = imread(path).astype(np.float32)            # (bands,H,W)\n",
        "        img_np = arr[[1,2,3],:,:].transpose(1,2,0).astype(np.uint8)\n",
        "        img    = self.transform(img_np)                     # → [3,224,224]\n",
        "\n",
        "        # original target at its native size\n",
        "        tar_np = arr[0,:,:]\n",
        "        target = torch.tensor(tar_np, dtype=torch.float32).unsqueeze(0)   # [1,H,W]\n",
        "        # resize to 224×224 so it matches your head’s output\n",
        "        target = F.interpolate(\n",
        "            target.unsqueeze(0), size=(224,224),\n",
        "            mode='bilinear', align_corners=False\n",
        "        ).squeeze(0)                                          # [1,224,224]\n",
        "\n",
        "        weather = torch.tensor(row[self.weather_cols].values.astype(np.float32))\n",
        "        return img, weather, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UTjOXA-avOV"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Read CSV, split, and create DataLoaders\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/PatchedOutput/tiff_with_meteo.csv\")\n",
        "for col in weather_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "df = df.dropna(subset=weather_cols + [\"filename\"]).reset_index(drop=True)\n",
        "\n",
        "patches_dir = \"/content/drive/MyDrive/PatchedOutput_Cleaned\"\n",
        "dataset     = LSTDataset(df, patches_dir, weather_cols)\n",
        "train_sz    = int(0.8 * len(dataset))\n",
        "val_sz      = len(dataset) - train_sz\n",
        "train_ds, val_ds = random_split(dataset, [train_sz, val_sz])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=0, pin_memory=False)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=0, pin_memory=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1RWwKKoaysX",
        "outputId": "ee282074-73e9-4bfe-e564-3330a8447590"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Optimizer, loss function, scheduler\n",
        "opt       = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4, weight_decay=1e-2\n",
        ")\n",
        "loss_fn   = nn.SmoothL1Loss()\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    opt, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR6rJcp_a05P",
        "outputId": "2e38ba42-fbde-45c6-86ba-68a28d612257"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Device setup & model instantiation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model  = PretrainedViTLSTModel(\n",
        "    weather_dim=len(weather_cols),\n",
        "    hidden_dim=768,\n",
        "    vit_name=\"vit_base_patch16_224\",\n",
        "    num_layers=2,\n",
        "    num_heads=8\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH5Mi7WVa33d",
        "outputId": "dca45cc4-4be6-46dd-f483-5d44d0d6517a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 00 Train: 100%|██████████| 1429/1429 [05:14<00:00,  4.54it/s, avg_loss=4.6777, batch_loss=4.9709]\n",
            "Epoch 00   Val : 100%|██████████| 358/358 [01:04<00:00,  5.51it/s, avg_loss=4.7516, batch_loss=3.0185]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00 | Train: 4.6777 | Val: 4.7516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 01 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.55it/s, avg_loss=4.6767, batch_loss=3.6956]\n",
            "Epoch 01   Val : 100%|██████████| 358/358 [01:05<00:00,  5.46it/s, avg_loss=4.7568, batch_loss=3.3660]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train: 4.6767 | Val: 4.7568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 02 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.56it/s, avg_loss=4.6833, batch_loss=4.4240]\n",
            "Epoch 02   Val : 100%|██████████| 358/358 [01:05<00:00,  5.49it/s, avg_loss=4.7533, batch_loss=3.6487]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02 | Train: 4.6833 | Val: 4.7533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 03 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.56it/s, avg_loss=4.6897, batch_loss=3.5375]\n",
            "Epoch 03   Val : 100%|██████████| 358/358 [01:04<00:00,  5.52it/s, avg_loss=4.7309, batch_loss=3.1766]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03 | Train: 4.6897 | Val: 4.7309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 04 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.55it/s, avg_loss=4.6853, batch_loss=6.0677]\n",
            "Epoch 04   Val : 100%|██████████| 358/358 [01:04<00:00,  5.51it/s, avg_loss=4.7438, batch_loss=3.6821]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04 | Train: 4.6853 | Val: 4.7438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 05 Train: 100%|██████████| 1429/1429 [05:14<00:00,  4.54it/s, avg_loss=4.6773, batch_loss=5.7809]\n",
            "Epoch 05   Val : 100%|██████████| 358/358 [01:04<00:00,  5.54it/s, avg_loss=4.7562, batch_loss=3.5703]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05 | Train: 4.6773 | Val: 4.7562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 06 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.55it/s, avg_loss=4.6789, batch_loss=4.0838]\n",
            "Epoch 06   Val : 100%|██████████| 358/358 [01:04<00:00,  5.52it/s, avg_loss=4.7504, batch_loss=2.8590]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06 | Train: 4.6789 | Val: 4.7504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 07 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.55it/s, avg_loss=4.6822, batch_loss=3.8228]\n",
            "Epoch 07   Val : 100%|██████████| 358/358 [01:04<00:00,  5.55it/s, avg_loss=4.7257, batch_loss=3.7118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07 | Train: 4.6822 | Val: 4.7257\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 08 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.55it/s, avg_loss=4.6753, batch_loss=6.2062]\n",
            "Epoch 08   Val : 100%|██████████| 358/358 [01:04<00:00,  5.55it/s, avg_loss=4.7334, batch_loss=3.4046]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08 | Train: 4.6753 | Val: 4.7334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 09 Train: 100%|██████████| 1429/1429 [05:14<00:00,  4.55it/s, avg_loss=4.6748, batch_loss=8.2943]\n",
            "Epoch 09   Val : 100%|██████████| 358/358 [01:04<00:00,  5.55it/s, avg_loss=4.7353, batch_loss=3.5355]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09 | Train: 4.6748 | Val: 4.7353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10 Train: 100%|██████████| 1429/1429 [05:11<00:00,  4.58it/s, avg_loss=4.6699, batch_loss=3.9576]\n",
            "Epoch 10   Val : 100%|██████████| 358/358 [01:04<00:00,  5.57it/s, avg_loss=4.7527, batch_loss=2.8998]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Train: 4.6699 | Val: 4.7527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11 Train: 100%|██████████| 1429/1429 [05:12<00:00,  4.57it/s, avg_loss=4.6747, batch_loss=3.3215]\n",
            "Epoch 11   Val : 100%|██████████| 358/358 [01:04<00:00,  5.52it/s, avg_loss=4.7523, batch_loss=2.8771]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | Train: 4.6747 | Val: 4.7523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.56it/s, avg_loss=4.6735, batch_loss=3.1250]\n",
            "Epoch 12   Val : 100%|██████████| 358/358 [01:04<00:00,  5.57it/s, avg_loss=4.7473, batch_loss=2.8887]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | Train: 4.6735 | Val: 4.7473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13 Train: 100%|██████████| 1429/1429 [05:13<00:00,  4.56it/s, avg_loss=4.6704, batch_loss=4.3612]\n",
            "Epoch 13   Val : 100%|██████████| 358/358 [01:04<00:00,  5.55it/s, avg_loss=4.7352, batch_loss=2.8172]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | Train: 4.6704 | Val: 4.7352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14 Train: 100%|██████████| 1429/1429 [05:12<00:00,  4.57it/s, avg_loss=4.6704, batch_loss=4.4716]\n",
            "Epoch 14   Val : 100%|██████████| 358/358 [01:04<00:00,  5.54it/s, avg_loss=4.7438, batch_loss=2.9095]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | Train: 4.6704 | Val: 4.7438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15 Train: 100%|██████████| 1429/1429 [05:12<00:00,  4.58it/s, avg_loss=4.6747, batch_loss=3.3528]\n",
            "Epoch 15   Val : 100%|██████████| 358/358 [01:04<00:00,  5.58it/s, avg_loss=4.7635, batch_loss=2.8907]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 | Train: 4.6747 | Val: 4.7635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16 Train: 100%|██████████| 1429/1429 [05:12<00:00,  4.57it/s, avg_loss=4.6795, batch_loss=4.6411]\n",
            "Epoch 16   Val : 100%|██████████| 358/358 [01:04<00:00,  5.56it/s, avg_loss=4.7341, batch_loss=2.9712]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 | Train: 4.6795 | Val: 4.7341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17 Train: 100%|██████████| 1429/1429 [05:12<00:00,  4.57it/s, avg_loss=4.6723, batch_loss=4.0108]\n",
            "Epoch 17   Val : 100%|██████████| 358/358 [01:04<00:00,  5.55it/s, avg_loss=4.7540, batch_loss=3.0447]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 | Train: 4.6723 | Val: 4.7540\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18 Train: 100%|██████████| 1429/1429 [05:11<00:00,  4.58it/s, avg_loss=4.6745, batch_loss=3.9277]\n",
            "Epoch 18   Val : 100%|██████████| 358/358 [01:04<00:00,  5.58it/s, avg_loss=4.7448, batch_loss=2.8143]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 | Train: 4.6745 | Val: 4.7448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19 Train: 100%|██████████| 1429/1429 [05:11<00:00,  4.59it/s, avg_loss=4.6865, batch_loss=6.6359]\n",
            "Epoch 19   Val : 100%|██████████| 358/358 [01:04<00:00,  5.56it/s, avg_loss=4.7551, batch_loss=3.7441]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 | Train: 4.6865 | Val: 4.7551\n",
            "✅ Training finished\n"
          ]
        }
      ],
      "source": [
        "# Cell 12: Training & validation loop WITH real‑time avg loss (fresh start)\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 20\n",
        "start_ep   = 0          # ← always start at 0\n",
        "\n",
        "for epoch in range(start_ep, num_epochs):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    train_loss    = 0.0\n",
        "    seen_samples  = 0\n",
        "    train_bar     = tqdm(train_loader, desc=f\"Epoch {epoch:02d} Train\")\n",
        "    for batch_idx, (imgs, weather, tgt) in enumerate(train_bar):\n",
        "        imgs, weather, tgt = imgs.to(device), weather.to(device), tgt.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out   = model(imgs, weather)\n",
        "        loss  = loss_fn(out, tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        # update running sums\n",
        "        batch_val     = loss.item()\n",
        "        n             = imgs.size(0)\n",
        "        train_loss   += batch_val * n\n",
        "        seen_samples += n\n",
        "        avg_train     = train_loss / seen_samples\n",
        "\n",
        "        train_bar.set_postfix(\n",
        "            batch_loss=f\"{batch_val:.4f}\",\n",
        "            avg_loss  =f\"{avg_train:.4f}\"\n",
        "        )\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    val_loss    = 0.0\n",
        "    seen_val    = 0\n",
        "    val_bar     = tqdm(val_loader, desc=f\"Epoch {epoch:02d}   Val \")\n",
        "    with torch.no_grad():\n",
        "        for imgs, weather, tgt in val_bar:\n",
        "            imgs, weather, tgt = imgs.to(device), weather.to(device), tgt.to(device)\n",
        "            out        = model(imgs, weather)\n",
        "            batch_val  = loss_fn(out, tgt).item()\n",
        "            n          = imgs.size(0)\n",
        "            val_loss  += batch_val * n\n",
        "            seen_val  += n\n",
        "            avg_val    = val_loss / seen_val\n",
        "\n",
        "            val_bar.set_postfix(\n",
        "                batch_loss=f\"{batch_val:.4f}\",\n",
        "                avg_loss  =f\"{avg_val:.4f}\"\n",
        "            )\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    print(f\"Epoch {epoch:02d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
        "    save_ckpt(epoch)\n",
        "\n",
        "print(\"✅ Training finished\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we try to unfreeze the last part of the vit"
      ],
      "metadata": {
        "id": "lpmQi-lxuzFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1ht-CYyX1jwkL6UaMAZNqiDrBIPZx8M21"
      ],
      "metadata": {
        "id": "K68WQqw3eUsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a quick breakdown of **why** we make each of those three tweaks and **what** they’re doing under the hood:\n",
        "\n",
        "1. **Unfreezing late ViT blocks**  \n",
        "   - **Why:** The ViT backbone was pretrained on ImageNet RGB images, which look very different from your multi‑band LST patches. By unfreezing just the last two transformer blocks and the final norm layer, you let the model adjust its high‑level feature detectors to your domain without blowing away all of its useful low‑level filters.  \n",
        "   - **What:** Those `blocks.10`, `blocks.11`, and `norm` layers become trainable, so during backpropagation their weights will shift to better capture patterns (e.g., thermal gradients, texture) in your satellite patches.\n",
        "\n",
        "2. **Richer upsampling head**  \n",
        "   - **Why:** A single `ConvTranspose2d` can only “blow up” your feature map once, which often leads to blurry or blocky outputs. A deeper sequence of smaller up‑ and down‑sampling steps with intermediate nonlinearities gives the network more capacity to reconstruct fine spatial details.  \n",
        "   - **What:** You replace  \n",
        "     ```python\n",
        "     ConvTranspose2d(hidden_dim → 1)\n",
        "     ```  \n",
        "     with  \n",
        "     ```python\n",
        "     ConvTranspose2d(hidden_dim → hidden_dim/2) → ReLU → ConvTranspose2d(hidden_dim/2 → hidden_dim/4) → ReLU → Conv2d(hidden_dim/4 → 1)\n",
        "     ```  \n",
        "     so the model gradually upsamples and refines features at each stage.\n",
        "\n",
        "3. **Injecting the [CLS] token back in**  \n",
        "   - **Why:** The `[CLS]` token in ViT holds a global summary of the entire image (all patches). If you discard it, your head only sees local patch embeddings plus weather. Re‑injecting it lets every patch token “know” the global context, which helps coordinate outputs across the whole map.  \n",
        "   - **What:** Instead of dropping `feats[:,0]`, you concatenate it with your patch tokens and the weather embedding—so your transformer fusion layer sees `(patches + weather + CLS)` as one sequence, letting global information flow back into each spatial location before you decode.\n",
        "\n",
        "Altogether, these changes let your pretrained ViT adapt its highest‑level concepts, decode richer spatial structure, and leverage both local and global context to produce cleaner, more accurate LST maps."
      ],
      "metadata": {
        "id": "dh3qg4sCfkXS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2hzVWAtfjPE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}