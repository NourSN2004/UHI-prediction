# -*- coding: utf-8 -*-
"""Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/157xE8wLwv9eBTzW5k5CXLKTRPuxbhNx2
"""

from google.colab import drive
drive.mount('/content/drive')

# Cell 0 ▶ Install required libraries
!pip install \
    imagecodecs \
    tifffile \
    rasterio \
    timm \
    segmentation-models-pytorch \
    matplotlib \
    tqdm

# Cell 1 ▶ Imports & Drive mount
import imagecodecs          # ensure TIFF LZW support
import os, math
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import segmentation_models_pytorch as smp
from tifffile import imread
from tqdm.auto import tqdm
from google.colab import drive

# Cell 2 ▶ Dataset for TransUNet testing (image + 224² LST target)
class LSTPatchDataset(Dataset):
    def __init__(self, df, patches_dir, transform):
        self.df          = df.reset_index(drop=True)
        self.patches_dir = patches_dir
        self.tfm         = transform

    def __len__(self): return len(self.df)

    def __getitem__(self, idx):
        row = self.df.loc[idx]
        arr = imread(os.path.join(self.patches_dir, row.patch_filename)
                    ).astype(np.float32)            # (4,H,W)

        img  = self.tfm(arr[[1,2,3]].transpose(1,2,0).astype(np.uint8))
        lst  = torch.tensor(arr[0], dtype=torch.float32).unsqueeze(0)
        lst  = F.interpolate(lst.unsqueeze(0), size=(224,224),
                             mode='bilinear', align_corners=False
                            ).squeeze(0)            # [1,224,224]
        lst  = torch.clamp(lst, 0.0, 1.0)           # match training scale
        return img, lst

# Cell 3 ▶ Build 2023 test DataLoader
CSV       = "/content/drive/MyDrive/test_patches/patch_with_meteo_2023_last6h.csv"
PATCH_DIR = "/content/drive/MyDrive/test_patches"

df_test = pd.read_csv(CSV)          # we only need patch_filename column
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

test_ds = LSTPatchDataset(df_test, PATCH_DIR, transform)
test_dl = DataLoader(test_ds,
                     batch_size=4,
                     shuffle=False,
                     num_workers=0,   # main proc ⇒ uses LZW codec
                     pin_memory=True)
print(f"Test patches: {len(test_ds)}")

# Cell 4 ▶ Re-create TransUNet model & load best weights
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = smp.Unet(
    encoder_name='mit_b0',     # same backbone as training
    encoder_weights=None,      # weights come from your .pth file
    in_channels=3,
    classes=1
).to(DEVICE)

ckpt_path = "/content/drive/MyDrive/best_transunet_ftl.pth"
model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))
model.eval()
print("✅ loaded checkpoint:", ckpt_path)



# Cell 5 ▶ Loss & quick evaluation on first 20 batches
def focal_tversky_loss(logits, targets,
                       alpha=0.7, beta=0.3, gamma=0.75, eps=1e-6):
    probs = torch.sigmoid(logits)
    dims  = (2,3)
    TP = (probs * targets).sum(dim=dims)
    FN = ((1-probs) * targets).sum(dim=dims)
    FP = (probs * (1-targets)).sum(dim=dims)
    tversky = (TP + eps) / (TP + alpha*FN + beta*FP + eps)
    return ((1 - tversky) ** gamma).mean()

max_batches = 50
ftl_total   = 0.0
rmse_total  = 0.0
pix_total   = 0

for imgs, masks in tqdm(
        iter(test_dl), total=max_batches, desc="Testing (20 batches)"
    ):
    if max_batches == 0: break
    imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)
    with torch.no_grad():
        preds = model(imgs)
    # FTL
    ftl = focal_tversky_loss(preds, masks).item()
    ftl_total += ftl

    # RMSE (after sigmoid because training logits weren’t clamped)
    preds_float = torch.sigmoid(preds)
    rmse_batch = ((preds_float - masks) ** 2).sum().item()
    rmse_total += rmse_batch
    pix_total  += masks.numel()

    max_batches -= 1
    if max_batches == 0:
        break

rmse = math.sqrt(rmse_total / pix_total)
print(f"\n▶ Avg Focal-Tversky on 50 batches: {ftl_total/20:.4f}")
print(f"▶ RMSE on 50 batches: {rmse:.3f} (scaled LST units)")